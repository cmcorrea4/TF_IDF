import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import re
from nltk.stem import SnowballStemmer
import numpy as np

# Download required NLTK data if needed
try:
    import nltk
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

st.set_page_config(page_title="Demo TF-IDF Espa√±ol", page_icon="üîç", layout="wide")

st.title("üîç Demo de TF-IDF en Espa√±ol con Preguntas y Respuestas")

st.markdown("""
### ¬øC√≥mo funciona?
Cada l√≠nea se trata como un **documento** (puede ser una frase, un p√°rrafo o un texto m√°s largo).  
La aplicaci√≥n utiliza:
- **TF-IDF**: Para calcular la importancia de cada palabra en cada documento
- **Stemming**: Para que palabras como *jugando* y *juegan* se consideren equivalentes
- **Similitud del coseno**: Para encontrar el documento m√°s relevante a tu pregunta
""")

# Sidebar con configuraciones
st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
min_df = st.sidebar.slider("Frecuencia m√≠nima de t√©rminos", 1, 5, 1, 
                          help="Ignora t√©rminos que aparecen en menos de N documentos")
max_features = st.sidebar.selectbox("M√°ximo n√∫mero de caracter√≠sticas", 
                                   [None, 100, 500, 1000], index=0)
similarity_threshold = st.sidebar.slider("Umbral de similitud", 0.0, 1.0, 0.1, 0.01,
                                        help="Documentos con similitud menor a este valor se marcar√°n como poco relevantes")

# Dos columnas para la interfaz principal
col1, col2 = st.columns([2, 1])

with col1:
    # Ejemplo inicial mejorado
    default_text = """El perro ladra fuerte en el parque.
El gato ma√∫lla suavemente durante la noche.
El perro y el gato juegan juntos en el jard√≠n.
Los ni√±os corren y se divierten en el parque.
La m√∫sica suena muy alta en la fiesta.
Los p√°jaros cantan hermosas melod√≠as al amanecer."""

    text_input = st.text_area(
        "üìù Escribe tus documentos (uno por l√≠nea):",
        default_text,
        height=200,
        help="Cada l√≠nea ser√° tratada como un documento separado"
    )

    question = st.text_input(
        "‚ùì Escribe una pregunta:", 
        "¬øQui√©n est√° jugando?",
        help="La aplicaci√≥n buscar√° el documento m√°s relevante para responder tu pregunta"
    )

with col2:
    st.markdown("### üí° Ejemplos de preguntas:")
    st.markdown("""
    - ¬øQui√©n est√° jugando?
    - ¬øQu√© animal hace ruido?
    - ¬øD√≥nde corren los ni√±os?
    - ¬øCu√°ndo cantan los p√°jaros?
    - ¬øQu√© hace el gato?
    """)

# Inicializar stemmer para espa√±ol
stemmer = SnowballStemmer("spanish")

def tokenize_and_stem(text: str):
    """
    Tokeniza y aplica stemming a un texto en espa√±ol.
    """
    # Pasar a min√∫sculas
    text = text.lower()
    # Eliminar caracteres no alfab√©ticos (mantener caracteres espa√±oles)
    text = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±\s]', ' ', text)
    # Tokenizar (palabras con longitud > 1)
    tokens = [t for t in text.split() if len(t) > 1]
    # Aplicar stemming
    stems = [stemmer.stem(t) for t in tokens]
    return stems

def highlight_matches(text: str, stems: list) -> str:
    """
    Resalta las palabras que coinciden con los stems en el texto.
    """
    words = text.split()
    highlighted = []
    for word in words:
        word_stem = stemmer.stem(re.sub(r'[^a-z√°√©√≠√≥√∫√º√±]', '', word.lower()))
        if word_stem in stems:
            highlighted.append(f"**{word}**")
        else:
            highlighted.append(word)
    return " ".join(highlighted)

# Bot√≥n principal
if st.button("üîç Analizar documentos y buscar respuesta", type="primary"):
    documents = [d.strip() for d in text_input.split("\n") if d.strip()]
    
    if len(documents) < 1:
        st.error("‚ö†Ô∏è Por favor, ingresa al menos un documento.")
    elif not question.strip():
        st.error("‚ö†Ô∏è Por favor, escribe una pregunta.")
    else:
        with st.spinner("Procesando documentos..."):
            # Configurar vectorizador
            vectorizer_params = {
                'tokenizer': tokenize_and_stem,
                # 'stop_words': 'spanish',  # Removido - manejamos stopwords en tokenize_and_stem
                'min_df': min_df,
            }
            if max_features:
                vectorizer_params['max_features'] = max_features
                
            vectorizer = TfidfVectorizer(**vectorizer_params)
            
            try:
                # Ajustar con documentos
                X = vectorizer.fit_transform(documents)
                
                # Vector de la pregunta
                question_vec = vectorizer.transform([question])
                
                # Similitud coseno
                similarities = cosine_similarity(question_vec, X).flatten()
                
                # Resultados
                st.success("‚úÖ An√°lisis completado!")
                
                # Crear tabs para organizar mejor la informaci√≥n
                tab1, tab2, tab3 = st.tabs(["üéØ Resultado Principal", "üìä An√°lisis Detallado", "üî¢ Matriz TF-IDF"])
                
                with tab1:
                    # Documento m√°s parecido
                    best_idx = similarities.argmax()
                    best_doc = documents[best_idx]
                    best_score = similarities[best_idx]
                    
                    st.markdown("### üéØ Resultado de b√∫squeda")
                    
                    col_q, col_a = st.columns(2)
                    with col_q:
                        st.markdown("**‚ùì Tu pregunta:**")
                        st.info(question)
                    
                    with col_a:
                        st.markdown("**üí° Documento m√°s relevante:**")
                        if best_score >= similarity_threshold:
                            # Resaltar palabras que coinciden
                            q_stems = tokenize_and_stem(question)
                            vocab = vectorizer.get_feature_names_out()
                            matched_stems = [s for s in q_stems if s in vocab]
                            highlighted_doc = highlight_matches(best_doc, matched_stems)
                            st.success(highlighted_doc)
                            st.caption(f"üìà Puntaje de similitud: {best_score:.3f}")
                        else:
                            st.warning(f"‚ö†Ô∏è {best_doc}")
                            st.caption(f"üìâ Similitud baja: {best_score:.3f} (< {similarity_threshold})")
                
                with tab2:
                    # Mostrar todas las similitudes
                    sim_df = pd.DataFrame({
                        "Documento": [f"Doc {i+1}" for i in range(len(documents))],
                        "Texto": documents,
                        "Similitud": similarities
                    })
                    sim_df = sim_df.sort_values("Similitud", ascending=False)
                    
                    st.markdown("### üìä Ranking de documentos")
                    
                    # Agregar colores basados en similitud
                    def color_similarity(val):
                        if val >= 0.5:
                            return 'background-color: #d4edda'  # Verde claro
                        elif val >= 0.2:
                            return 'background-color: #fff3cd'  # Amarillo claro
                        else:
                            return 'background-color: #f8d7da'  # Rojo claro
                    
                    styled_df = sim_df.style.applymap(color_similarity, subset=['Similitud'])
                    st.dataframe(styled_df, use_container_width=True)
                    
                    # Mostrar coincidencias de stems
                    vocab = vectorizer.get_feature_names_out()
                    q_stems = tokenize_and_stem(question)
                    matched = [s for s in q_stems if s in vocab and 
                              sim_df.iloc[0]['Similitud'] > 0]  # Del documento con mayor similitud
                    
                    if matched:
                        st.markdown("### üî§ T√©rminos coincidentes (despu√©s de stemming)")
                        st.info(f"T√©rminos encontrados: {', '.join(matched)}")
                    else:
                        st.warning("No se encontraron t√©rminos coincidentes despu√©s del procesamiento")
                
                with tab3:
                    # Mostrar matriz TF-IDF
                    df_tfidf = pd.DataFrame(
                        X.toarray(),
                        columns=vectorizer.get_feature_names_out(),
                        index=[f"Doc {i+1}" for i in range(len(documents))]
                    )
                    
                    st.markdown("### üî¢ Matriz TF-IDF")
                    st.caption("Valores mayores indican mayor importancia del t√©rmino en el documento")
                    
                    # Filtrar columnas con valores > 0 para mejor visualizaci√≥n
                    non_zero_cols = df_tfidf.columns[df_tfidf.sum() > 0]
                    if len(non_zero_cols) > 20:
                        st.warning(f"Mostrando solo las primeras 20 columnas de {len(non_zero_cols)} t√©rminos")
                        display_df = df_tfidf[non_zero_cols[:20]]
                    else:
                        display_df = df_tfidf[non_zero_cols]
                    
                    st.dataframe(display_df.round(3), use_container_width=True)
                    
                    # Estad√≠sticas del vocabulario
                    st.markdown("### üìà Estad√≠sticas del vocabulario")
                    col_stats1, col_stats2, col_stats3 = st.columns(3)
                    
                    with col_stats1:
                        st.metric("Documentos", len(documents))
                    with col_stats2:
                        st.metric("T√©rminos √∫nicos", len(vectorizer.get_feature_names_out()))
                    with col_stats3:
                        st.metric("Similitud m√°xima", f"{similarities.max():.3f}")
            
            except ValueError as e:
                st.error(f"‚ùå Error en el procesamiento: {str(e)}")
                st.info("üí° Intenta agregar m√°s documentos o usar palabras m√°s variadas")

# Informaci√≥n adicional en la sidebar
st.sidebar.markdown("---")
st.sidebar.markdown("### üìö Informaci√≥n t√©cnica")
st.sidebar.markdown("""
**TF-IDF** significa *Term Frequency - Inverse Document Frequency* 
y mide la importancia de una palabra en un documento dentro de una colecci√≥n.

**Stemming** reduce las palabras a su ra√≠z, por ejemplo:
- jugando ‚Üí jug
- juegan ‚Üí jug  
- jugador ‚Üí jug

**Similitud del coseno** mide qu√© tan similares son dos vectores, 
donde 1 = id√©nticos y 0 = completamente diferentes.
""")

st.sidebar.markdown("---")
st.sidebar.caption("Desarrollado  usando Streamlit y scikit-learn")
